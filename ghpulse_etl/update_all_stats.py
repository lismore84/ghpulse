#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
GHPulse å®Œæ•´ç»Ÿè®¡æ›´æ–°è„šæœ¬
æ›´æ–°æ‰€æœ‰ç»Ÿè®¡å’Œç¼“å­˜è¡¨ï¼Œé€‚åˆå®šæ—¶ä»»åŠ¡è¿è¡Œ

æ›´æ–°çš„è¡¨ï¼š
1. hot_repos - çƒ­é—¨ä»“åº“æ¦œå•
2. active_developers - æ´»è·ƒå¼€å‘è€…æ¦œå•
3. actor_stats_cache - ç”¨æˆ·ç»Ÿè®¡ç¼“å­˜
4. repo_stats_cache - ä»“åº“ç»Ÿè®¡ç¼“å­˜
5. event_stats_daily - æ¯æ—¥äº‹ä»¶ç»Ÿè®¡
6. base_stats - åŸºç¡€ç»Ÿè®¡æ•°æ®
"""

import pymysql
import os
from dotenv import load_dotenv
from datetime import datetime, timedelta
import logging
import sys

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.StreamHandler(sys.stdout),
        logging.FileHandler('stats_update.log', encoding='utf-8')
    ]
)
logger = logging.getLogger(__name__)

load_dotenv()

DB_CONFIG = {
    'host': os.getenv('DB_HOST'),
    'port': int(os.getenv('DB_PORT', 3306)),
    'user': os.getenv('DB_USER'),
    'password': os.getenv('DB_PASSWORD'),
    'database': os.getenv('DB_NAME'),
    'charset': 'utf8mb4'
}


def get_db_connection():
    """è·å–æ•°æ®åº“è¿æ¥"""
    try:
        return pymysql.connect(**DB_CONFIG)
    except Exception as e:
        logger.error(f"æ•°æ®åº“è¿æ¥å¤±è´¥: {e}")
        raise


def update_hot_repos():
    """æ›´æ–°çƒ­é—¨ä»“åº“æ¦œå•"""
    conn = get_db_connection()
    cursor = conn.cursor()
    
    try:
        logger.info("=" * 60)
        logger.info("ğŸ”¥ æ›´æ–°çƒ­é—¨ä»“åº“æ¦œå•")
        logger.info("=" * 60)
        
        # æ£€æŸ¥è¡¨æ˜¯å¦å­˜åœ¨
        cursor.execute("SHOW TABLES LIKE 'hot_repos'")
        if not cursor.fetchone():
            logger.error("âŒ hot_repos è¡¨ä¸å­˜åœ¨ï¼Œè¯·å…ˆè¿è¡Œåˆå§‹åŒ–è„šæœ¬")
            return
        
        cursor.execute("DELETE FROM hot_repos")
        deleted = cursor.rowcount
        logger.info(f"âœ“ æ¸…ç©ºæ—§æ•°æ®: {deleted} è¡Œ")
        
        logger.info("â³ è®¡ç®—çƒ­é—¨ä»“åº“ï¼ˆåŸºäºæ˜Ÿæ ‡ã€Forkã€PR æ´»è·ƒåº¦ï¼‰...")
        cursor.execute("""
            INSERT INTO hot_repos (
                repo_id, repo_name, score, 
                stars_7d, forks_7d, prs_7d, 
                rank_position, updated_at
            )
            SELECT 
                r.repo_id,
                r.name as repo_name,
                COALESCE(r.total_stars, 0) + 
                    COUNT(CASE WHEN e.event_type = 'WatchEvent' 
                        AND e.created_at >= DATE_SUB(NOW(), INTERVAL 7 DAY) THEN 1 END) * 2 +
                    COUNT(CASE WHEN e.event_type = 'ForkEvent' 
                        AND e.created_at >= DATE_SUB(NOW(), INTERVAL 7 DAY) THEN 1 END) * 1.5 as score,
                COUNT(CASE WHEN e.event_type = 'WatchEvent' 
                    AND e.created_at >= DATE_SUB(NOW(), INTERVAL 7 DAY) THEN 1 END) as stars_7d,
                COUNT(CASE WHEN e.event_type = 'ForkEvent' 
                    AND e.created_at >= DATE_SUB(NOW(), INTERVAL 7 DAY) THEN 1 END) as forks_7d,
                COUNT(CASE WHEN e.event_type = 'PullRequestEvent' 
                    AND e.created_at >= DATE_SUB(NOW(), INTERVAL 7 DAY) THEN 1 END) as prs_7d,
                ROW_NUMBER() OVER (ORDER BY 
                    COALESCE(r.total_stars, 0) + 
                    COUNT(CASE WHEN e.event_type = 'WatchEvent' 
                        AND e.created_at >= DATE_SUB(NOW(), INTERVAL 7 DAY) THEN 1 END) * 2 +
                    COUNT(CASE WHEN e.event_type = 'ForkEvent' 
                        AND e.created_at >= DATE_SUB(NOW(), INTERVAL 7 DAY) THEN 1 END) * 1.5
                    DESC
                ) as rank_position,
                NOW() as updated_at
            FROM repos r
            LEFT JOIN events e ON r.repo_id = e.repo_id
            GROUP BY r.repo_id, r.name, r.total_stars
            HAVING score > 0
            ORDER BY score DESC
            LIMIT 100
        """)
        
        count = cursor.rowcount
        conn.commit()
        logger.info(f"âœ“ æˆåŠŸæ’å…¥ {count} ä¸ªçƒ­é—¨ä»“åº“")
        
        # æ˜¾ç¤º Top 3
        cursor.execute("""
            SELECT rank_position, repo_name, score, stars_7d, forks_7d, prs_7d
            FROM hot_repos ORDER BY rank_position LIMIT 3
        """)
        logger.info("\nğŸ“Š Top 3 çƒ­é—¨ä»“åº“:")
        for row in cursor.fetchall():
            logger.info(f"  #{row[0]} {row[1]} - å¾—åˆ†:{row[2]:.1f} â­7æ—¥:{row[3]} ğŸ´7æ—¥:{row[4]}")
        
    except Exception as e:
        logger.error(f"âŒ æ›´æ–°å¤±è´¥: {e}")
        import traceback
        logger.error(traceback.format_exc())
        conn.rollback()
    finally:
        cursor.close()
        conn.close()


def update_active_developers():
    """æ›´æ–°æ´»è·ƒå¼€å‘è€…æ¦œå•"""
    conn = get_db_connection()
    cursor = conn.cursor()
    
    try:
        logger.info("=" * 60)
        logger.info("ğŸ‘¨â€ğŸ’» æ›´æ–°æ´»è·ƒå¼€å‘è€…æ¦œå•")
        logger.info("=" * 60)
        
        cursor.execute("SHOW TABLES LIKE 'active_developers'")
        if not cursor.fetchone():
            logger.error("âŒ active_developers è¡¨ä¸å­˜åœ¨")
            return
        
        cursor.execute("DELETE FROM active_developers")
        deleted = cursor.rowcount
        logger.info(f"âœ“ æ¸…ç©ºæ—§æ•°æ®: {deleted} è¡Œ")
        
        logger.info("â³ è®¡ç®—æ´»è·ƒå¼€å‘è€…ï¼ˆåŸºäºæäº¤ã€PRã€Issue æ´»è·ƒåº¦ï¼‰...")
        cursor.execute("""
            INSERT INTO active_developers (
                actor_id, actor_login, activity_score,
                commits_7d, prs_7d, issues_7d, repos_7d,
                rank_position, updated_at
            )
            SELECT 
                a.actor_id,
                a.login as actor_login,
                COALESCE(a.total_events, 0) + 
                    COUNT(CASE WHEN e.created_at >= DATE_SUB(NOW(), INTERVAL 7 DAY) THEN 1 END) as activity_score,
                COUNT(CASE WHEN e.event_type = 'PushEvent' 
                    AND e.created_at >= DATE_SUB(NOW(), INTERVAL 7 DAY) THEN 1 END) as commits_7d,
                COUNT(CASE WHEN e.event_type = 'PullRequestEvent' 
                    AND e.created_at >= DATE_SUB(NOW(), INTERVAL 7 DAY) THEN 1 END) as prs_7d,
                COUNT(CASE WHEN e.event_type = 'IssuesEvent' 
                    AND e.created_at >= DATE_SUB(NOW(), INTERVAL 7 DAY) THEN 1 END) as issues_7d,
                COUNT(DISTINCT CASE WHEN e.created_at >= DATE_SUB(NOW(), INTERVAL 7 DAY) THEN e.repo_id END) as repos_7d,
                ROW_NUMBER() OVER (ORDER BY 
                    COALESCE(a.total_events, 0) + 
                    COUNT(CASE WHEN e.created_at >= DATE_SUB(NOW(), INTERVAL 7 DAY) THEN 1 END) 
                    DESC
                ) as rank_position,
                NOW() as updated_at
            FROM actors a
            LEFT JOIN events e ON a.actor_id = e.actor_id
            GROUP BY a.actor_id, a.login, a.total_events
            HAVING activity_score > 0
            ORDER BY activity_score DESC
            LIMIT 100
        """)
        
        count = cursor.rowcount
        conn.commit()
        logger.info(f"âœ“ æˆåŠŸæ’å…¥ {count} ä¸ªæ´»è·ƒå¼€å‘è€…")
        
        # æ˜¾ç¤º Top 3
        cursor.execute("""
            SELECT rank_position, actor_login, activity_score, commits_7d, prs_7d, repos_7d
            FROM active_developers ORDER BY rank_position LIMIT 3
        """)
        logger.info("\nğŸ“Š Top 3 æ´»è·ƒå¼€å‘è€…:")
        for row in cursor.fetchall():
            logger.info(f"  #{row[0]} {row[1]} - å¾—åˆ†:{row[2]} ğŸ“7æ—¥æäº¤:{row[3]} ğŸ”€PR:{row[4]} ğŸ“¦ä»“åº“:{row[5]}")
        
    except Exception as e:
        logger.error(f"âŒ æ›´æ–°å¤±è´¥: {e}")
        conn.rollback()
    finally:
        cursor.close()
        conn.close()


def update_actor_stats_cache():
    """æ›´æ–°ç”¨æˆ·ç»Ÿè®¡ç¼“å­˜ï¼ˆå…¨é‡ï¼‰"""
    conn = get_db_connection()
    cursor = conn.cursor()
    
    try:
        logger.info("=" * 60)
        logger.info("ğŸ“ˆ æ›´æ–°ç”¨æˆ·ç»Ÿè®¡ç¼“å­˜")
        logger.info("=" * 60)
        
        cursor.execute("SHOW TABLES LIKE 'actor_stats_cache'")
        if not cursor.fetchone():
            logger.error("âŒ actor_stats_cache è¡¨ä¸å­˜åœ¨")
            return
        
        cursor.execute("DELETE FROM actor_stats_cache")
        deleted = cursor.rowcount
        logger.info(f"âœ“ æ¸…ç©ºæ—§æ•°æ®: {deleted} è¡Œ")
        
        logger.info("â³ è®¡ç®—ç”¨æˆ·ç»Ÿè®¡ï¼ˆå¯èƒ½éœ€è¦å‡ åˆ†é’Ÿï¼‰...")
        cursor.execute("""
            INSERT INTO actor_stats_cache (
                actor_id,
                total_commits,
                total_prs,
                total_issues,
                total_repos,
                total_stars_received,
                commits_7d,
                prs_7d,
                repos_7d,
                updated_at
            )
            SELECT 
                a.actor_id,
                COUNT(CASE WHEN e.event_type = 'PushEvent' THEN 1 END) as total_commits,
                COUNT(CASE WHEN e.event_type = 'PullRequestEvent' THEN 1 END) as total_prs,
                COUNT(CASE WHEN e.event_type = 'IssuesEvent' THEN 1 END) as total_issues,
                COUNT(DISTINCT e.repo_id) as total_repos,
                COUNT(CASE WHEN e.event_type = 'WatchEvent' THEN 1 END) as total_stars_received,
                COUNT(CASE 
                    WHEN e.event_type = 'PushEvent' 
                    AND e.created_at >= DATE_SUB(NOW(), INTERVAL 7 DAY) 
                    THEN 1 
                END) as commits_7d,
                COUNT(CASE 
                    WHEN e.event_type = 'PullRequestEvent' 
                    AND e.created_at >= DATE_SUB(NOW(), INTERVAL 7 DAY) 
                    THEN 1 
                END) as prs_7d,
                COUNT(DISTINCT CASE 
                    WHEN e.created_at >= DATE_SUB(NOW(), INTERVAL 7 DAY) 
                    THEN e.repo_id 
                END) as repos_7d,
                NOW() as updated_at
            FROM actors a
            LEFT JOIN events e ON a.actor_id = e.actor_id
            GROUP BY a.actor_id
            HAVING total_commits > 0 OR total_prs > 0 OR total_issues > 0
        """)
        
        count = cursor.rowcount
        conn.commit()
        logger.info(f"âœ“ æˆåŠŸæ’å…¥ {count} ä¸ªç”¨æˆ·ç»Ÿè®¡")
        
    except Exception as e:
        logger.error(f"âŒ æ›´æ–°å¤±è´¥: {e}")
        conn.rollback()
    finally:
        cursor.close()
        conn.close()


def update_repo_stats_cache():
    """æ›´æ–°ä»“åº“ç»Ÿè®¡ç¼“å­˜ï¼ˆå…¨é‡ï¼‰"""
    conn = get_db_connection()
    cursor = conn.cursor()
    
    try:
        logger.info("=" * 60)
        logger.info("ğŸ“¦ æ›´æ–°ä»“åº“ç»Ÿè®¡ç¼“å­˜")
        logger.info("=" * 60)
        
        # æ£€æŸ¥è¡¨æ˜¯å¦å­˜åœ¨
        cursor.execute("SHOW TABLES LIKE 'repo_stats_cache'")
        if not cursor.fetchone():
            logger.warning("âš ï¸  repo_stats_cache è¡¨ä¸å­˜åœ¨ï¼Œè·³è¿‡")
            return
        
        cursor.execute("DELETE FROM repo_stats_cache")
        deleted = cursor.rowcount
        logger.info(f"âœ“ æ¸…ç©ºæ—§æ•°æ®: {deleted} è¡Œ")
        
        logger.info("â³ è®¡ç®—ä»“åº“ç»Ÿè®¡ï¼ˆå¯èƒ½éœ€è¦å‡ åˆ†é’Ÿï¼‰...")
        
        # æ ¹æ®å®é™…è¡¨ç»“æ„ï¼Œä» repos å’Œ events èšåˆæ•°æ®
        cursor.execute("""
            INSERT INTO repo_stats_cache (
                repo_id,
                total_stars,
                total_forks,
                total_watchers,
                total_contributors,
                total_commits,
                total_prs,
                total_issues,
                stars_1d,
                stars_7d,
                stars_30d,
                updated_at
            )
            SELECT 
                r.repo_id,
                
                -- å†å²ç´¯è®¡æ•°æ®ï¼ˆä» repos è¡¨ç›´æ¥è¯»å–ï¼‰
                COALESCE(r.total_stars, 0) as total_stars,
                COALESCE(r.total_forks, 0) as total_forks,
                
                -- total_watchers ä» events è®¡ç®—ï¼ˆWatchEvent çš„å”¯ä¸€ç”¨æˆ·æ•°ï¼‰
                COUNT(DISTINCT CASE WHEN e.event_type = 'WatchEvent' THEN e.actor_id END) as total_watchers,
                
                -- total_contributors ä» repos è¡¨æˆ– events è®¡ç®—
                GREATEST(
                    COALESCE(r.total_contributors, 0),
                    COUNT(DISTINCT e.actor_id)
                ) as total_contributors,
                
                -- ä» events èšåˆçš„ç»Ÿè®¡
                COUNT(CASE WHEN e.event_type = 'PushEvent' THEN 1 END) as total_commits,
                COUNT(CASE WHEN e.event_type = 'PullRequestEvent' THEN 1 END) as total_prs,
                COUNT(CASE WHEN e.event_type = 'IssuesEvent' THEN 1 END) as total_issues,
                
                -- è¿‘æœŸæ˜Ÿæ ‡å¢é‡
                COUNT(CASE 
                    WHEN e.event_type = 'WatchEvent' 
                    AND e.created_at >= DATE_SUB(NOW(), INTERVAL 1 DAY) 
                    THEN 1 
                END) as stars_1d,
                COUNT(CASE 
                    WHEN e.event_type = 'WatchEvent' 
                    AND e.created_at >= DATE_SUB(NOW(), INTERVAL 7 DAY) 
                    THEN 1 
                END) as stars_7d,
                COUNT(CASE 
                    WHEN e.event_type = 'WatchEvent' 
                    AND e.created_at >= DATE_SUB(NOW(), INTERVAL 30 DAY) 
                    THEN 1 
                END) as stars_30d,
                
                NOW() as updated_at
                
            FROM repos r
            LEFT JOIN events e ON r.repo_id = e.repo_id
            GROUP BY r.repo_id, r.total_stars, r.total_forks, r.total_contributors
            HAVING total_commits > 0 OR total_prs > 0 OR total_issues > 0 OR stars_7d > 0
        """)
        
        count = cursor.rowcount
        conn.commit()
        logger.info(f"âœ“ æˆåŠŸæ’å…¥ {count} ä¸ªä»“åº“ç»Ÿè®¡")
        
        # æ˜¾ç¤ºç»Ÿè®¡æ‘˜è¦
        cursor.execute("""
            SELECT 
                COUNT(*) as total_repos,
                SUM(total_stars) as sum_stars,
                SUM(total_watchers) as sum_watchers,
                SUM(stars_7d) as sum_stars_7d,
                MAX(total_stars) as max_stars
            FROM repo_stats_cache
        """)
        row = cursor.fetchone()
        if row and row[0] > 0:
            logger.info(f"\nğŸ“Š ä»“åº“ç»Ÿè®¡æ‘˜è¦:")
            logger.info(f"  æ€»ä»“åº“æ•°: {row[0]:,}")
            logger.info(f"  æ€»æ˜Ÿæ ‡æ•°: {row[1]:,}")
            logger.info(f"  æ€»å…³æ³¨æ•°: {row[2]:,}")
            logger.info(f"  7æ—¥æ–°å¢æ˜Ÿæ ‡: {row[3]:,}")
            logger.info(f"  æœ€é«˜æ˜Ÿæ ‡: {row[4]:,}")
        
        # æ˜¾ç¤º Top 3 ä»“åº“
        cursor.execute("""
            SELECT repo_id, total_stars, total_watchers, stars_7d, total_commits, total_prs
            FROM repo_stats_cache
            ORDER BY total_stars DESC
            LIMIT 3
        """)
        logger.info("\nğŸ“Š Top 3 ä»“åº“ï¼ˆæŒ‰æ˜Ÿæ ‡ï¼‰:")
        for row in cursor.fetchall():
            logger.info(f"  repo_id:{row[0]} â­{row[1]:,} ğŸ‘€{row[2]:,} (7æ—¥+{row[3]}) ğŸ“{row[4]:,}æäº¤ ğŸ”€{row[5]:,}PR")
        
    except Exception as e:
        logger.error(f"âŒ æ›´æ–°å¤±è´¥: {e}")
        import traceback
        logger.error(traceback.format_exc())
        conn.rollback()
    finally:
        cursor.close()
        conn.close()


def update_base_statistics():
    """æ›´æ–°åŸºç¡€ç»Ÿè®¡æ•°æ®ï¼ˆactorsã€reposè¡¨çš„ç»Ÿè®¡ä¿¡æ¯ï¼‰"""
    conn = get_db_connection()
    cursor = conn.cursor()
    
    try:
        logger.info("=" * 60)
        logger.info("ğŸ“Š æ›´æ–°åŸºç¡€ç»Ÿè®¡æ•°æ®")
        logger.info("=" * 60)
        
        # æ›´æ–°actorsç»Ÿè®¡
        logger.info("  æ›´æ–°ç”¨æˆ·ç»Ÿè®¡...")
        cursor.execute("""
            UPDATE actors a
            INNER JOIN (
                SELECT 
                    actor_id,
                    MAX(created_at) AS last_active,
                    COUNT(*) AS event_count
                FROM events
                GROUP BY actor_id
            ) e ON a.actor_id = e.actor_id
            SET 
                a.last_active_at = GREATEST(COALESCE(a.last_active_at, '1970-01-01'), e.last_active),
                a.total_events = a.total_events + e.event_count
        """)
        logger.info(f"    æ›´æ–°äº† {cursor.rowcount} ä¸ªç”¨æˆ·")
        
        # æ›´æ–°reposç»Ÿè®¡
        logger.info("  æ›´æ–°ä»“åº“ç»Ÿè®¡...")
        cursor.execute("""
            UPDATE repos r
            INNER JOIN (
                SELECT 
                    repo_id,
                    MAX(created_at) AS last_event,
                    COUNT(*) AS event_count,
                    SUM(CASE WHEN event_type = 'WatchEvent' THEN 1 ELSE 0 END) AS stars,
                    SUM(CASE WHEN event_type = 'ForkEvent' THEN 1 ELSE 0 END) AS forks
                FROM events
                GROUP BY repo_id
            ) e ON r.repo_id = e.repo_id
            SET 
                r.last_event_at = GREATEST(COALESCE(r.last_event_at, '1970-01-01'), e.last_event),
                r.total_events = r.total_events + e.event_count,
                r.total_stars = r.total_stars + e.stars,
                r.total_forks = r.total_forks + e.forks
        """)
        logger.info(f"    æ›´æ–°äº† {cursor.rowcount} ä¸ªä»“åº“")
        
        # æ›´æ–°ç”¨æˆ·-ä»“åº“å…³è”
        logger.info("  æ›´æ–°ç”¨æˆ·-ä»“åº“å…³è”...")
        cursor.execute("""
            INSERT INTO user_repo_relation (
                actor_id, repo_id, relation_type, relation_time,
                first_event_at, last_event_at, event_count
            )
            SELECT 
                e.actor_id,
                e.repo_id,
                CASE 
                    WHEN e.event_type = 'WatchEvent' THEN 'star'
                    WHEN e.event_type = 'ForkEvent' THEN 'fork'
                    ELSE 'contributor'
                END AS relation_type,
                MIN(e.created_at) AS relation_time,
                MIN(e.created_at) AS first_event_at,
                MAX(e.created_at) AS last_event_at,
                COUNT(*) AS event_count
            FROM events e
            INNER JOIN actors a ON e.actor_id = a.actor_id  -- ç¡®ä¿actorå­˜åœ¨
            INNER JOIN repos r ON e.repo_id = r.repo_id      -- ç¡®ä¿repoå­˜åœ¨
            GROUP BY e.actor_id, e.repo_id, 
                CASE 
                    WHEN e.event_type = 'WatchEvent' THEN 'star'
                    WHEN e.event_type = 'ForkEvent' THEN 'fork'
                    ELSE 'contributor'
                END
            ON DUPLICATE KEY UPDATE
                last_event_at = VALUES(last_event_at),
                event_count = event_count + VALUES(event_count)
        """)
        logger.info(f"    æ›´æ–°äº† {cursor.rowcount} æ¡å…³è”")
        
        conn.commit()
        logger.info("  âœ“ åŸºç¡€ç»Ÿè®¡æ•°æ®æ›´æ–°å®Œæˆ")
        
    except Exception as e:
        logger.error(f"  âœ— åŸºç¡€ç»Ÿè®¡æ›´æ–°å¤±è´¥: {e}")
        import traceback
        logger.error(traceback.format_exc())
        conn.rollback()
    finally:
        cursor.close()
        conn.close()


def update_event_stats_daily(days=30):
    """
    æ›´æ–°æ¯æ—¥äº‹ä»¶ç»Ÿè®¡
    
    Args:
        days: æ›´æ–°æœ€è¿‘å‡ å¤©çš„æ•°æ®ï¼ˆé»˜è®¤30å¤©ï¼‰
    """
    conn = get_db_connection()
    cursor = conn.cursor()
    
    try:
        logger.info("=" * 60)
        logger.info(f"ğŸ“… æ›´æ–°æ¯æ—¥äº‹ä»¶ç»Ÿè®¡ï¼ˆæœ€è¿‘ {days} å¤©ï¼‰")
        logger.info("=" * 60)
        
        # æ£€æŸ¥è¡¨æ˜¯å¦å­˜åœ¨
        cursor.execute("SHOW TABLES LIKE 'event_stats_daily'")
        if not cursor.fetchone():
            logger.error("âŒ event_stats_daily è¡¨ä¸å­˜åœ¨")
            return
        
        # åˆ é™¤æœ€è¿‘Nå¤©çš„æ•°æ®ï¼Œé‡æ–°è®¡ç®—
        start_date = (datetime.now() - timedelta(days=days)).strftime('%Y-%m-%d')
        cursor.execute("""
            DELETE FROM event_stats_daily 
            WHERE stats_date >= %s
        """, (start_date,))
        deleted = cursor.rowcount
        logger.info(f"âœ“ æ¸…ç©ºæœ€è¿‘ {days} å¤©çš„æ—§æ•°æ®: {deleted} è¡Œ")
        
        logger.info("â³ è®¡ç®—æ¯æ—¥äº‹ä»¶ç»Ÿè®¡...")
        
        # ä½¿ç”¨æ­£ç¡®çš„å­—æ®µåï¼štotal_count, unique_actors, unique_repos, unique_orgs, stats_time
        cursor.execute("""
            INSERT INTO event_stats_daily (
                stats_date,
                event_type,
                total_count,
                unique_actors,
                unique_repos,
                unique_orgs,
                stats_time
            )
            SELECT 
                DATE(e.created_at) as stats_date,
                e.event_type,
                COUNT(*) as total_count,
                COUNT(DISTINCT e.actor_id) as unique_actors,
                COUNT(DISTINCT e.repo_id) as unique_repos,
                COUNT(DISTINCT e.org_id) as unique_orgs,
                NOW() as stats_time
            FROM events e
            WHERE e.created_at >= DATE_SUB(CURDATE(), INTERVAL %s DAY)
            GROUP BY DATE(e.created_at), e.event_type
            ORDER BY stats_date DESC, total_count DESC
        """, (days,))
        
        count = cursor.rowcount
        conn.commit()
        logger.info(f"âœ“ æˆåŠŸæ’å…¥ {count} æ¡æ¯æ—¥ç»Ÿè®¡è®°å½•")
        
        # æ˜¾ç¤ºæœ€è¿‘3å¤©çš„ç»Ÿè®¡æ‘˜è¦ï¼ˆä½¿ç”¨æ­£ç¡®çš„å­—æ®µåï¼‰
        cursor.execute("""
            SELECT 
                stats_date,
                SUM(total_count) as total_events,
                COUNT(DISTINCT event_type) as event_types,
                SUM(unique_actors) as total_actors,
                SUM(unique_repos) as total_repos
            FROM event_stats_daily
            WHERE stats_date >= DATE_SUB(CURDATE(), INTERVAL 3 DAY)
            GROUP BY stats_date
            ORDER BY stats_date DESC
            LIMIT 3
        """)
        
        logger.info("\nğŸ“Š æœ€è¿‘3å¤©äº‹ä»¶ç»Ÿè®¡æ‘˜è¦:")
        for row in cursor.fetchall():
            logger.info(f"  {row[0]} - äº‹ä»¶:{row[1]:,} | ç±»å‹:{row[2]} | ç”¨æˆ·:{row[3]:,} | ä»“åº“:{row[4]:,}")
        
    except Exception as e:
        logger.error(f"âŒ æ›´æ–°å¤±è´¥: {e}")
        import traceback
        logger.error(traceback.format_exc())
        conn.rollback()
    finally:
        cursor.close()
        conn.close()


def show_summary():
    """æ˜¾ç¤ºæ‰€æœ‰ç»Ÿè®¡è¡¨çš„æ‘˜è¦"""
    conn = get_db_connection()
    cursor = conn.cursor()
    
    try:
        logger.info("\n" + "=" * 60)
        logger.info("ğŸ“Š ç»Ÿè®¡è¡¨æ‘˜è¦")
        logger.info("=" * 60)
        
        tables = [
            ('hot_repos', 'çƒ­é—¨ä»“åº“æ¦œå•'),
            ('active_developers', 'æ´»è·ƒå¼€å‘è€…æ¦œå•'),
            ('actor_stats_cache', 'ç”¨æˆ·ç»Ÿè®¡ç¼“å­˜'),
            ('repo_stats_cache', 'ä»“åº“ç»Ÿè®¡ç¼“å­˜'),
            ('event_stats_daily', 'æ¯æ—¥äº‹ä»¶ç»Ÿè®¡')
        ]
        
        for table, name in tables:
            try:
                cursor.execute(f"SELECT COUNT(*) FROM {table}")
                count = cursor.fetchone()[0]
                
                # è·å–æœ€åæ›´æ–°æ—¶é—´
                try:
                    # event_stats_daily è¡¨ç”¨ stats_timeï¼Œå…¶ä»–è¡¨ç”¨ updated_at
                    time_column = 'stats_time' if table == 'event_stats_daily' else 'updated_at'
                    cursor.execute(f"""
                        SELECT MAX({time_column}) FROM {table} 
                        WHERE {time_column} IS NOT NULL
                    """)
                    last_update = cursor.fetchone()[0]
                    if last_update:
                        time_str = last_update.strftime('%Y-%m-%d %H:%M:%S')
                    else:
                        time_str = 'æœªçŸ¥'
                except:
                    time_str = 'N/A'
                
                logger.info(f"âœ“ {name:20s}: {count:8,d} æ¡ | æœ€åæ›´æ–°: {time_str}")
            except Exception as e:
                logger.warning(f"âš ï¸  {name:20s}: æŸ¥è¯¢å¤±è´¥ ({e})")
        
    except Exception as e:
        logger.error(f"æ˜¾ç¤ºæ‘˜è¦å¤±è´¥: {e}")
    finally:
        cursor.close()
        conn.close()


def main():
    """ä¸»å‡½æ•° - æ‰§è¡Œæ‰€æœ‰ç»Ÿè®¡æ›´æ–°"""
    
    start_time = datetime.now()
    
    logger.info("\n" + " ğŸš€ " + "=" * 58)
    logger.info(" ğŸš€ GHPulse ç»Ÿè®¡æ›´æ–°ä»»åŠ¡å¼€å§‹")
    logger.info(" ğŸš€ " + "=" * 58)
    logger.info(f"å¼€å§‹æ—¶é—´: {start_time.strftime('%Y-%m-%d %H:%M:%S')}")
    logger.info("æ›´æ–°èŒƒå›´: æ‰€æœ‰ç»Ÿè®¡æ•°æ®")
    logger.info("")
    
    # æ— æ¡ä»¶æ›´æ–°æ‰€æœ‰ç»Ÿè®¡æ•°æ®
    update_hot_repos()
    update_active_developers()
    update_actor_stats_cache()
    update_repo_stats_cache()
    update_event_stats_daily(30)  # é»˜è®¤æ›´æ–°30å¤©çš„æ¯æ—¥ç»Ÿè®¡
    update_base_statistics()  # æ›´æ–°åŸºç¡€ç»Ÿè®¡æ•°æ®
    
    # æ˜¾ç¤ºæ‘˜è¦
    show_summary()
    
    elapsed = (datetime.now() - start_time).total_seconds()
    
    logger.info("\n" + "=" * 60)
    logger.info(f"âœ“ ç»Ÿè®¡æ›´æ–°å®Œæˆï¼è€—æ—¶: {elapsed:.2f} ç§’")
    logger.info("=" * 60)
    logger.info("\nğŸ’¡ æç¤º:")
    logger.info("  - å¯è®¾ç½®å®šæ—¶ä»»åŠ¡æ¯å°æ—¶è¿è¡Œ: 0 * * * * python update_all_stats.py")
    logger.info("=" * 60)


if __name__ == '__main__':
    try:
        main()
    except KeyboardInterrupt:
        logger.warning("\nâš ï¸  ç”¨æˆ·ä¸­æ–­æ‰§è¡Œ")
        sys.exit(1)
    except Exception as e:
        logger.error(f"\nâŒ æ‰§è¡Œå¤±è´¥: {e}")
        import traceback
        logger.error(traceback.format_exc())
        sys.exit(1)